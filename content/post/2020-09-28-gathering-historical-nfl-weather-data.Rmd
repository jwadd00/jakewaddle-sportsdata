---
title: Gathering Historical NFL Weather Data
author: Jake Waddle
date: '2020-09-28'
slug: gathering-historical-nfl-weather-data
categories:
  - R
tags:
  - NFL
  - Weather
  - Fantasy
description: ''
topics: []
---

# Warmish Welcome

Fantasy sports are a passion of mine because they reside at the intersection of two of my favorite things in the world: sports and data. Over the last 4 or so years, I have built data sets suitable for modeling fantasy football and basketball outcomes. I tend to favor daily fantasy, but weather data is valuable for season long fantasy football as well.  

In fantasy football, weather data are valuable for a variety of reasons. Conventional wisdom states temperature, wind, and precipation will affect throwing the football, the kicking game, holding on to the football, etc. After reading through this post, you will have the necessary data to explore these claims. 

## Setting Up

To start, we'll load the necessary libraries. From there, we'll find a website with weather data, write a function to scrape the data and then run that function on several web pages automatically to build a complete historical data set. 

The website we will be using is [NFL Weather](http://nflweather.com/). Like most real-world exercises, the data is not presented to us in a perfectly built table. We'll need to do a fair amount of data wrangling to mold the data into a useable structure. 


```{r Setting Up, message = FALSE, warning = FALSE}
# package management
library(tidyverse)
library(rvest)
library(janitor)
```


# Exploring NFLWeather.com

Once you navigate to [NFL Weather](http://nflweather.com/), you can see the weeks indexed with small blue boxes towards the top, directly in the middle of the page. Clicking on the '1' takes you to the weather for week 1 of the 2020 season. Notice the url for this page: http://nflweather.com/en/week/2020/week-1/. Simply replace '2020' in the url with '2019' will direct you to the week 1 forecast for the 2019 season. As you might have guessed, swapping 'week-1' with 'week-8' will launch the forecast for week 8. Already, we can see that the forecast for each week of a given season can be accessed by switching the year and week parts of the url.

Let's start with accessing the weather data for week 1 of the 2020 season. Using rvest, we can quickly scan the html for tables and parse appropriately using the `read_html()` and `html_table()` functions. These commands parse the html from the url given and return the tables in a list. Using `glimpse()`, we see that a list of 1 data frame that has some blank rows and null columns is returned.


```{r Single Page Scrape, message = FALSE, warning = FALSE}
url <- 'http://nflweather.com/en/week/2020/week-1/'

url %>% 
  read_html() %>% 
  html_table() %>%
  glimpse()
```


## Data Wrangling

We've returned the data frame we want, but definitely not the data frame we need. The initial output requires additional work. 

We'll use `clean_names()` from the `janitor` package to give each column a nice, clean name.


```{r Single Page Data Wrangle 01, message = FALSE, warning = FALSE}
url %>% 
  read_html() %>% 
  html_table() %>%
  .[[1]] %>%  # select the first and only element in the list 
  clean_names() %>%
  glimpse()
```


From there, we'll select both home and away team columns, the forecast column, and the wind column. The forecast field holds a text description of the weather from which we can parse the temperature and precipiation out of. 


```{r Single Page Data Wrangle 02, message = FALSE, warning = FALSE}
url %>% 
  read_html() %>% 
  html_table() %>%
  .[[1]] %>% 
  clean_names() %>%
  select(away, home, forecast, wind) %>% 
  glimpse()
```


## Text Gymnastics

Since the season and week fields are not included in the data, we need to extract them from the url. Remembering back to when we changed the '2020' in the url to '2019', we know that the position of the season portion of the url will not change. Using `stringr` and `str_sub`, we're able to extract characters based on their position within the url - in this case characters 31-34. 

Extracting the week out of the url is a little more complex. To start, we use regular expressions to extract all characters of the url following the '-'. Using `sub()`, we actually pattern match all characters prior to and including the '-' and replace with '' (nothing). This leaves us with `1/`. From here, we just need to capture the number to serve as our week column. Here, I go back to `str_sub` and extract the characters from the right beginning with 3 characters from the right and ending with 2 characters from the right which will capture double digit weeks as well. 


```{r String Olympics, message = FALSE, warning = FALSE}
url %>% 
  read_html() %>% 
  html_table() %>%
  .[[1]] %>% 
  clean_names() %>%
  select(away, home, forecast, wind) %>%
  mutate(season = str_sub(url, start= 31, end = 34),
         week = str_sub(sub('.*\\-', '', url), start = -3, end = -2)) %>% 
  glimpse()
```


For capturing the wind, we'll simply extract all characters prior to the lower case 'm' in the wind column. This will give us the numerical representation of the wind. Similarly, for temperature we can use the same regular expression to extract all characters prior to the lower case 'f' in the forecast column.  


```{r Extract Weather, message = FALSE, warning = FALSE}
url %>% 
  read_html() %>% 
  html_table() %>%
  .[[1]] %>% 
  clean_names() %>%
  select(away, home, forecast, wind) %>%
  mutate(season = str_sub(url, start= 31, end = 34),
         week = str_sub(sub('.*\\-', '', url), start = -3, end = -2),
         wind = as.numeric(gsub( "m.*$", "",wind)),
         temperature = ifelse(forecast == 'DOME', 71, gsub( "f.*$", "", forecast)),
         weather = sub(".*? (.+)", "\\1", forecast)) %>%
  glimpse()
```
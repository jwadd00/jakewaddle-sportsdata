---
title: Gathering Historical NFL Weather Data
author: Jake Waddle
date: '2020-09-28'
slug: gathering-historical-nfl-weather-data
categories:
  - R
tags:
  - NFL
  - Weather
  - Fantasy
description: ''
topics: []
---

# Warmish Welcome

Fantasy sports are a passion of mine because they reside at the intersection of two of my favorite things in the world: sports and data. Over the last 4 or so years, I have built data sets suitable for modeling fantasy football and basketball outcomes. I tend to favor daily fantasy, but weather data is valuable for season long fantasy football as well.  

In fantasy football, weather data are valuable for a variety of reasons. Conventional wisdom states temperature, wind, and precipation will affect throwing the football, the kicking game, holding on to the football, etc. After reading through this post, you will have the necessary data to explore these claims. 

## Setting Up

To start, we'll load the necessary libraries. From there, we'll find a website with weather data, write a function to scrape the data and then run that function on several web pages automatically to build a complete historical data set. 

The website we will be using is [NFL Weather](http://nflweather.com/). Like most real-world exercises, the data is not presented to us in a perfectly built table. We'll need to do a fair amount of data wrangling to mold the data into a useable structure. 

```{r Hidden Setups for Website, include = FALSE, warning = FALSE, message = FALSE}
library(knitr)
library(kableExtra)
```
```{r Setting Up, message = FALSE, warning = FALSE}
# package management
library(tidyverse)
library(rvest)
library(janitor)
library(glue)
```


# Exploring NFLWeather.com

Once you navigate to [NFL Weather](http://nflweather.com/), you can see the weeks indexed with small blue boxes towards the top, directly in the middle of the page. Clicking on the '1' takes you to the weather for week 1 of the 2020 season. Notice the url for this page: http://nflweather.com/en/week/2020/week-1/. Simply replace '2020' in the url with '2019' will direct you to the week 1 forecast for the 2019 season. As you might have guessed, swapping 'week-1' with 'week-8' will launch the forecast for week 8. Already, we can see that the forecast for each week of a given season can be accessed by switching the year and week parts of the url.

Let's start with accessing the weather data for week 1 of the 2020 season. Using rvest, we can quickly scan the html for tables and parse appropriately using the `read_html()` and `html_table()` functions. These commands parse the html from the url given and return the tables in a list. Using `glimpse()`, we see that a list of 1 data frame that has some blank rows and null columns is returned.


```{r Single Page Scrape, message = FALSE, warning = FALSE}
url <- 'http://nflweather.com/en/week/2020/week-1/'

url %>% 
  read_html() %>% 
  html_table() %>%
  glimpse()
```


## Data Wrangling

We've returned the data frame we want, but definitely not the data frame we need. The initial output requires additional work. 

We'll use `clean_names()` from the `janitor` package to give each column a nice, clean name.


```{r Single Page Data Wrangle 01, message = FALSE, warning = FALSE}
url %>% 
  read_html() %>% 
  html_table() %>%
  .[[1]] %>%  # select the first and only element in the list 
  clean_names() %>%
  kable(format = 'html') %>% 
  kable_styling()
```


From there, we'll select both home and away team columns, the forecast column, and the wind column. The forecast field holds a text description of the weather from which we can parse the temperature and precipiation out of. 


```{r Single Page Data Wrangle 02, message = FALSE, warning = FALSE}
url %>% 
  read_html() %>% 
  html_table() %>%
  .[[1]] %>% 
  clean_names() %>%
  select(away, home, forecast, wind) %>% 
  kable(format = 'html') %>% 
  kable_styling()
```


## Text Gymnastics

Since the season and week fields are not included in the data, we need to extract them from the url. Remembering back to when we changed the '2020' in the url to '2019', we know that the position of the season portion of the url will not change. Using `stringr` and `str_sub`, we're able to extract characters based on their position within the url - in this case characters 31-34. 

Extracting the week out of the url is a little more complex. To start, we use regular expressions to extract all characters of the url following the '-'. Using `sub()`, we actually pattern match all characters prior to and including the '-' and replace with '' (nothing). This leaves us with `1/`. From here, we just need to capture the number to serve as our week column. Here, I go back to `str_sub` and extract the characters from the right beginning with 3 characters from the right and ending with 2 characters from the right which will capture double digit weeks as well. 


```{r String Olympics, message = FALSE, warning = FALSE}
url %>% 
  read_html() %>% 
  html_table() %>%
  .[[1]] %>% 
  clean_names() %>%
  select(away, home, forecast, wind) %>%
  mutate(season = str_sub(url, start= 31, end = 34),
         week = str_sub(gsub('.*\\-', '', url), start = -3, end = -2)) %>% 
  kable(format = 'html') %>% 
  kable_styling()
```


For capturing the wind, we'll simply remove all characters following and including the lower case 'm' in the wind column. This will give us the numerical representation of the wind. Similarly, for temperature we can use the same regular expression for the lower case 'f' in the forecast column. The weather column will be generated by removing every character prior to and including the first white space using the forecast column. 


```{r Extract Weather, message = FALSE, warning = FALSE}
url %>% 
  read_html() %>% 
  html_table() %>%
  .[[1]] %>% 
  clean_names() %>%
  select(away, home, forecast, wind) %>%
  mutate(season = str_sub(url, start= 31, end = 34),
         week = str_sub(gsub('.*\\-', '', url), start = -3, end = -2),
         wind = as.numeric(gsub( "m.*$", "",wind)),
         temperature = ifelse(forecast == 'DOME', 71, gsub( "f.*$", "", forecast)),
         weather = gsub(".*? ", "", forecast)) %>%
  kable(format = 'html') %>% 
  kable_styling()
```


## Pivot Action

At this point, the data is tidy'd enough to start using. However, before we move on to writing a function to scrape the data for us across multiple web pages at a time, we need to alter the orientation of the data. As is, there is a row for each game. Thinking bigger picture, we will want a row for each team - so that each row is unique for season, week, and team. Suppose you want to join or merge this weather data with a team's box score data from one or multiple games? This final transformation will make that task much easier. 

With the `pivot_longer()` function, we can elongate the data using the home and away columns - renaming the pivoted values column simply to 'team'. For example, if the Texans and Chiefs played, this function ensures there is a row for each team along with the corresponding weather data from the remaining columns. What you'll notice is that instead of 16 rows like the previous outputs, there will now be 32 (1 for each team instead of 1 for each game). The result is a clean dataset with a row for each season, week, team.


```{r Pivot, message = FALSE, warning = FALSE}
url %>% 
  read_html() %>% 
  html_table() %>%
  .[[1]] %>% 
  clean_names() %>%
  select(away, home, forecast, wind) %>%
  mutate(season = str_sub(url, start= 31, end = 34),
         week = str_sub(gsub('.*\\-', '', url), start = -3, end = -2),
         wind = as.numeric(gsub( "m.*$", "",wind)),
         temperature = ifelse(forecast == 'DOME', 71, gsub( "f.*$", "", forecast)),
         weather = gsub(".*? ", "", forecast)) %>%
  pivot_longer(cols = c('away', 'home'), values_to = 'team') %>% 
  select(-name, -forecast) %>% 
  select(team, season, week, temperature, wind, weather) %>% 
  kable(format = 'html') %>% 
  kable_styling()
```


# Mapping the Function

The final objective here is to use this code as a function. Functions can be used over and over without rewriting code. In this example, we can use and reuse our code for existing urls, grabbing data from a particular week of a particular season - or we can run for all weeks from multiple seasons. For this blog post, we're going to grab weather data for weeks 1 through 17 from 2018 and 2019. 

We actually need two functions. One for generating the web page urls and another for scraping / wrangling the data. For generating the urls, we establish the sequence of weeks and years. Then, with the help of `tidyr` and the `crossing()` function, we can generate all combinations of weeks and years. The result is a tibble with two columns containing a row for weeks 1-17 for each season 2018 and 2019. The function that generates the urls simply concatenates the base url with the year and week provided. Using `pmap`, we can iterate over each row of the `weather_weeks_and_years` tibble, passing the year and week from each row into our `generate_urls` function. The result is a list of 85 urls or web pages that we can now scrape data from.  


```{r Generate URLs, message = FALSE, warning = FALSE}
# set up weeks and years
weeks <- c(1:17)
years <- c(2018:2019)
weather_weeks_and_years <- crossing(year = years, week = weeks) # generate all unique combinations

# function to generate urls
generate_urls <- function(year, week) {
  full_url <- glue::glue("http://nflweather.com/en/week/{year}/week-{week}/")
}

# pass weeks and years through function
url_list <- pmap(weather_weeks_and_years, generate_urls)

head(url_list)
```


Below, we create a function that accepts a url, scrapes, wrangles, and returns the data. The code is the same code written above, used repeatedly for each url. We execute the function for each url within `url_list` by using `map_df` from the `purrr` package. `map_df` accepts an input list and applies a function to each element while returning a data frame. It might take a minute or two to run. Upon completion, you should have a solid weather dataset to start digging into. 


```{r Scrape Data Function, message = FALSE, warning = FALSE}
# function to scrape weather
scrape_weather_data <- function(webpage_url) {
  webpage_url %>% 
    read_html() %>% 
    html_table() %>%
    .[[1]] %>% 
    clean_names() %>%
    select(away, home, forecast, wind) %>%
    mutate(season = str_sub(webpage_url, start= 31, end = 34),
           week = str_sub(gsub('.*\\-', '', webpage_url), start = -3, end = -2),
           wind = as.numeric(gsub( "m.*$", "",wind)),
           temperature = ifelse(forecast == 'DOME', 71, gsub( "f.*$", "", forecast)),
           weather = gsub(".*? ", "", forecast)) %>%
    pivot_longer(cols = c('away', 'home'), values_to = 'team') %>% 
    select(-name, -forecast) %>% 
    select(team, season, week, temperature, wind, weather)
}

# pass urls through the function
weather_data <- map_df(url_list, scrape_weather_data)

weather_data %>% 
  sample_n(10) %>% 
  kable(format = 'html') %>% 
  kable_styling()
```


# Finally

That was a lot. From regular expressions, to creating functions, and using `purrr` - there were quite a few different tasks described in this post. In the wild, most data are messy and need to be gathered / cleaned using a wide variety of tricks and trades. Good bye. 
